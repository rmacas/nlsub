{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1580bb1",
   "metadata": {},
   "source": [
    "# NLSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5670a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as sstats\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a013589",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 512  # sampling rate\n",
    "\n",
    "def load_data(channels):\n",
    "    \n",
    "    dset = []\n",
    "    norm_factors = []\n",
    "    \n",
    "    for channel in channels:\n",
    "        \n",
    "        # load data\n",
    "        fname = f'data/{channel}' \n",
    "        data = np.load(fname)\n",
    "        \n",
    "        # remove sides due to whitening artifacts\n",
    "        gps_times = data[fs*4:-fs*4, 0]\n",
    "        data_tseries = data[fs*4:-fs*4, 1].reshape(-1, 1)\n",
    "        \n",
    "        # normalize and append tseries\n",
    "        norm_factor = np.max(np.abs(data_tseries))\n",
    "        norm_factors.append(norm_factor)\n",
    "        dset.append(data_tseries / norm_factor)\n",
    "    \n",
    "    dset = np.squeeze(dset)\n",
    "    dset = np.float32(dset)\n",
    "    gps_times = np.array(gps_times)\n",
    "    \n",
    "    return dset, gps_times, norm_factors\n",
    "\n",
    "\n",
    "# load training data\n",
    "channels = ['DCS-CALIB_STRAIN_CLEAN_C01_512Hz_27hrs_whitened.npy',\n",
    "            'LSC-POP_A_RF45_I_ERR_DQ_512Hz_27hrs_whitened.npy',\n",
    "            'LSC-POP_A_RF45_Q_ERR_DQ_512Hz_27hrs_whitened.npy',\n",
    "            'LSC-POP_A_RF9_I_ERR_DQ_512Hz_27hrs_whitened.npy']\n",
    "\n",
    "dset, gps_times, norm_factors = load_data(channels)\n",
    "\n",
    "\n",
    "# load event data\n",
    "channels_event = ['DCS-CALIB_STRAIN_CLEAN_C01_512Hz_event_120s_whitened.npy',\n",
    "            'LSC-POP_A_RF45_I_ERR_DQ_512Hz_event_120s_whitened.npy',\n",
    "            'LSC-POP_A_RF45_Q_ERR_DQ_512Hz_event_120s_whitened.npy',\n",
    "            'LSC-POP_A_RF9_I_ERR_DQ_512Hz_event_120s_whitened.npy']\n",
    "\n",
    "dset_event, gps_times_event, norm_factors_event = load_data(channels_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bf5f7",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3a8b4",
   "metadata": {},
   "source": [
    "### Select noisy times\n",
    "Algorithm fails to learn when all 27hrs of data is given because it's mostly Gaussian. Thus here we find times when an aux channel is non-Gaussian, and use these noisy times to get intput/output arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2361b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_threshold = 11.7  # threshold factor found by trial\n",
    "noise_idx = np.argwhere(np.abs(sstats.zscore(dset[1])) > z_threshold).T.tolist()[0]\n",
    "\n",
    "clear_size = fs * 4  # amount of data around to be considered 'related' around a data point\n",
    "\n",
    "# taken from https://stackoverflow.com/questions/53177358/removing-numbers-which-are-close-to-each-other-in-a-list\n",
    "usedValues = set()\n",
    "noise_idx_isolated = []\n",
    "\n",
    "for v in noise_idx:\n",
    "    if v not in usedValues:\n",
    "        noise_idx_isolated.append(v)\n",
    "\n",
    "        for lv in range(v - clear_size, v + clear_size+1):\n",
    "            usedValues.add(lv)\n",
    "\n",
    "print(f'{len(noise_idx_isolated)} noisy places')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d9f81",
   "metadata": {},
   "source": [
    "### Create input/output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input filter params for the DNN\n",
    "rec_size = 768\n",
    "rec_future = 256\n",
    "rec_size = 48\n",
    "rec_future = 16\n",
    "\n",
    "def get_arrays(dset, input_values, output_values, box_start, box_end, rec_size, rec_future):\n",
    "    rec_past = rec_size - rec_future\n",
    "    for i in range(box_start+rec_past, box_end-rec_future):\n",
    "        array = np.array([dset[1, i-rec_past:i+rec_future],\n",
    "                          dset[2, i-rec_past:i+rec_future],\n",
    "                          dset[3, i-rec_past:i+rec_future]])\n",
    "        #array = np.array([dset[1, i-rec_past:i+rec_future]])\n",
    "        input_values.append(array)\n",
    "    output_values.append(dset[0, box_start+rec_past:box_end-rec_future])\n",
    "    \n",
    "    return input_values, output_values\n",
    "\n",
    "input_values = []\n",
    "output_values = []\n",
    "\n",
    "for idx in noise_idx_isolated:\n",
    "    box_start = idx - clear_size\n",
    "    box_end = idx + clear_size\n",
    "    input_values, output_values = get_arrays(dset, input_values, output_values, box_start,\n",
    "                                             box_end, rec_size, rec_future)\n",
    "    \n",
    "output_values = np.hstack(output_values).reshape(-1,1)\n",
    "input_values = np.array(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_values.shape, input_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a4428",
   "metadata": {},
   "source": [
    "### Remove glitches\n",
    "Need also to remove times when the strain data contains huge glitches which are not caused by our aux channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_threshold = 40  # threshold factor found by trial\n",
    "glitches_idx = np.argwhere(np.abs(sstats.zscore(output_values)) > glitch_threshold).T.tolist()[0]\n",
    "\n",
    "# taken from https://stackoverflow.com/questions/53177358/removing-numbers-which-are-close-to-each-other-in-a-list\n",
    "usedValues = set()\n",
    "glitches_idx_isolated = []\n",
    "\n",
    "for v in glitches_idx:\n",
    "    if v not in usedValues:\n",
    "        glitches_idx_isolated.append(v)\n",
    "\n",
    "        for lv in range(v - clear_size, v + clear_size+1):\n",
    "            usedValues.add(lv)\n",
    "\n",
    "for idx in glitches_idx_isolated[::-1]:  # use REVERSED list\n",
    "    \n",
    "    idx_start = idx - clear_size - rec_size - rec_future\n",
    "    idx_end = idx + clear_size + rec_future\n",
    "        \n",
    "    output_values = np.delete(output_values, slice(idx_start, idx_end), axis = 0)\n",
    "    input_values = np.delete(input_values, slice(idx_start, idx_end), axis = 0)\n",
    "    \n",
    "print(f'Removed {len(glitches_idx_isolated)} glitches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840c157",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "After selecting noisy aux parts and removing times where strain data contains huge glitches, we need to re-normalize the data.\n",
    "\n",
    "Input values (i.e. aux channel data) don't need normalization because they contain the noisiest parts, so np.max(np.abs(..)) is already 1. Also note that the event data needs to be normalized with respect to the training data set, so that these two data sets would be consistent with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd90f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor_output = np.max(np.abs(output_values))\n",
    "output_values = output_values / norm_factor_output\n",
    "# note that input values dont need to be normalised since all of them have max val of 1\n",
    "\n",
    "# normalize event data\n",
    "dset_event_norm = []\n",
    "dset_event_norm.append(dset_event[0] * norm_factors_event[0]/norm_factors[0]/norm_factor_output)\n",
    "dset_event_norm.append(dset_event[1] * norm_factors_event[1]/norm_factors[1])\n",
    "dset_event_norm.append(dset_event[2] * norm_factors_event[2]/norm_factors[2])\n",
    "dset_event_norm.append(dset_event[3] * norm_factors_event[3]/norm_factors[3])\n",
    "dset_event_norm = np.array(dset_event_norm)\n",
    "\n",
    "# normalisation to be used to keep event and 27hrs datasets consistent; also used when coloring the data\n",
    "normalisation_values = [norm_factors[0]*norm_factor_output, norm_factors[1], norm_factors[2], norm_factors[3]]\n",
    "print(normalisation_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed86599",
   "metadata": {},
   "source": [
    "Create input/output arrays for the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b595b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values_event = []\n",
    "output_values_event = []\n",
    "input_values_event, output_values_event = get_arrays(dset_event_norm, input_values_event, \n",
    "                                                     output_values_event, 0, \n",
    "                                                     len(dset_event_norm[0,:]), rec_size, \n",
    "                                                     rec_future)\n",
    "\n",
    "input_values_event = np.array(input_values_event)\n",
    "output_values_event = np.hstack(output_values_event).reshape(-1,1)\n",
    "output_gps_times_event = gps_times_event[rec_size-rec_future:-rec_future]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337e4dc",
   "metadata": {},
   "source": [
    "### Get training, validation and testing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51023425",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "train_input, test_input = train_test_split(input_values, test_size=0.1, shuffle=True, random_state=seed)\n",
    "train_output, test_output = train_test_split(output_values, test_size=0.1, shuffle=True, random_state=seed)\n",
    "test_input = input_values_event\n",
    "test_output = output_values_event\n",
    "\n",
    "print('Training dset input/output shapes: ', train_input.shape, train_output.shape)\n",
    "print('Validating dset input/output shapes: ', valid_input.shape, valid_output.shape)\n",
    "print('Testing dset input/output shapes: ', test_input.shape, test_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89c9e3",
   "metadata": {},
   "source": [
    "## Creating and training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d574fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1024\n",
    "l2 = 1e-4\n",
    "batch = 10000\n",
    "epochs = 500\n",
    "lrate = 1e-3\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  lrate,\n",
    "  decay_steps=80,\n",
    "  decay_rate=10,\n",
    "  staircase=True)\n",
    "\n",
    "# MODEL\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(width, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(width, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# COMPILE AND FIT\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule))\n",
    "    \n",
    "model_fit = model.fit(train_input, train_output, validation_data=(valid_input, valid_output), batch_size = batch, epochs = epochs, verbose = 2)\n",
    "    \n",
    "# MAKE PREDICTION\n",
    "prediction = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09709bed",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_suffix = 'model'\n",
    "\n",
    "# PLOTTING\n",
    "gps = 1264316116 + 40\n",
    "plot_win = 50\n",
    "start_plot = gps - plot_win\n",
    "end_plot = gps + plot_win\n",
    "tseries_orig = TimeSeries(np.squeeze(output_values_event), times=output_gps_times_event)\n",
    "tseries_orig = tseries_orig.crop(start_plot,end_plot)\n",
    "tseries_pred = TimeSeries(np.squeeze(prediction), times=output_gps_times_event)\n",
    "tseries_pred = tseries_pred.crop(start_plot,end_plot)\n",
    "tseries_clean = TimeSeries(np.squeeze(output_values_event) - np.squeeze(prediction), times=output_gps_times_event)\n",
    "tseries_clean = tseries_clean.crop(start_plot,end_plot)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(tseries_pred, label='predicted')\n",
    "plt.legend()\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(tseries_orig, label='real')\n",
    "plt.plot(tseries_pred, label='predicted')\n",
    "plt.legend()\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(tseries_clean, label='real - predicted')\n",
    "plt.legend()\n",
    "plt.savefig(f'output/plots/training_{output_suffix}_tseries.png')\n",
    "\n",
    "# -------------------------------------\n",
    "def make_oscan(gps, oscan_name):\n",
    "    plot_win = 5\n",
    "    start_plot = gps - plot_win\n",
    "    end_plot = gps + plot_win\n",
    "\n",
    "    dataset = ['orig','clean','diff']\n",
    "    q_trans = {}\n",
    "    q_trans['orig'] = tseries_orig.q_transform(outseg=(start_plot,end_plot),qrange=(10,20))\n",
    "    q_trans['clean'] = tseries_clean.q_transform(outseg=(start_plot,end_plot),qrange=(10,20))\n",
    "    q_trans['diff'] = q_trans['orig'] - q_trans['clean']\n",
    "\n",
    "    ylim = (20, 200) \n",
    "    alim = (0, 25)\n",
    "\n",
    "    label = {}\n",
    "    label['orig'] = 'Original data'\n",
    "    label['clean'] = 'Cleaned data'\n",
    "    label['diff'] = 'Original - Cleaned'\n",
    "\n",
    "    plot, axes = plt.subplots(nrows=3, sharex=True, figsize=(3.375*2.0,3.375*3.0))\n",
    "\n",
    "    for i, ax in zip(dataset,axes):\n",
    "\n",
    "        pcm = ax.imshow(q_trans[i],vmin=alim[0],vmax=alim[1])\n",
    "        ax.set_ylim(ylim[0],ylim[1])\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_yscale('log')\n",
    "        ax.plot([gps],10, label=label[i], visible=False)\n",
    "        ax.grid(alpha=0.6)\n",
    "        ax.legend(loc='upper left', handlelength=0, handletextpad=0)\n",
    "\n",
    "    axes[1].set_ylabel(r\"$\\mathrm{Frequency \\ (Hz)}$\")\n",
    "    cbar = axes[0].colorbar(clim=(alim[0], alim[1]),location='top')\n",
    "    cbar.set_label(r\"$\\mathrm{Normalized \\ energy}$\")\n",
    "    plt.savefig(f'output/plots/{oscan_name}.png')\n",
    "\n",
    "gps = 1264316122\n",
    "oscan_name = f'training_{output_suffix}_oscan_1'\n",
    "make_oscan(gps, oscan_name)\n",
    "gps = 1264316154\n",
    "oscan_name = f'training_{output_suffix}_oscan_2'\n",
    "make_oscan(gps, oscan_name)\n",
    "gps = 1264316164\n",
    "oscan_name = f'training_{output_suffix}_oscan_3'\n",
    "make_oscan(gps, oscan_name)\n",
    "\n",
    "# -------------------------------------\n",
    "plt.figure()\n",
    "plt.semilogy(model_fit.history['loss'], label='train')\n",
    "plt.semilogy(model_fit.history['val_loss'], label='valid')\n",
    "plt.savefig(f'output/plots/training_{output_suffix}_hist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf568954",
   "metadata": {},
   "source": [
    "# Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'output/{output_suffix}')\n",
    "\n",
    "# save various params, including the normalisation values\n",
    "json_dict = {'norm_values': normalisation_values, 'fs': fs, 'rec_size':rec_size, 'rec_future':rec_future}\n",
    "with open(f'output/{output_suffix}/dataset_params.json', 'w') as outfile:                                    \n",
    "    json.dump(json_dict, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
